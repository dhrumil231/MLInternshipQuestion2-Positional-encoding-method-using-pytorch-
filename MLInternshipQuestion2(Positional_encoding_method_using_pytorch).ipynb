{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka0Vm6J3Y6zo",
        "outputId": "757d09f5-22a1-4ddd-d2f1-f49d305318df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "nhoXT--4bKL-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LEARNABLE POSITIONAL ENCODING CLASS**"
      ],
      "metadata": {
        "id": "pZWjdjpkcX1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, max_seq_len: int = 512, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Dimension of the model (embedding dimension)\n",
        "            max_seq_len: Maximum sequence length the model will handle\n",
        "            dropout: Dropout rate applied after adding positional encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Learnable position embeddings - each position gets a d_model-dimensional vector\n",
        "        # Initialized with scaled normal distribution for stable training\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "        # Initialize with scaled values (common practice)\n",
        "        nn.init.normal_(self.pos_embedding.weight, mean=0, std=d_model ** -0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add learnable positional encoding to input embeddings.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            Tensor of same shape with positional information added\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Verify sequence length is within bounds\n",
        "        if seq_len > self.max_seq_len:\n",
        "            raise ValueError(f\"Sequence length {seq_len} exceeds max_seq_len {self.max_seq_len}\")\n",
        "\n",
        "        # Create position indices: [0, 1, 2, ..., seq_len-1]\n",
        "        positions = torch.arange(seq_len, device=x.device)  # [seq_len]\n",
        "\n",
        "        # Get position embeddings and add to input\n",
        "        # pos_embedding(positions) -> [seq_len, d_model]\n",
        "        # Broadcasting adds this to each batch element\n",
        "        pos_enc = self.pos_embedding(positions)  # [seq_len, d_model]\n",
        "\n",
        "        # Add positional encoding to input embeddings\n",
        "        # x: [batch_size, seq_len, d_model] + [seq_len, d_model] -> broadcasts correctly\n",
        "        output = x + pos_enc.unsqueeze(0)\n",
        "\n",
        "        return self.dropout(output)"
      ],
      "metadata": {
        "id": "sFD1WYjHcFvD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALTERNATIVE: LEARNABLE PE WITH nn.Parameter (More flexible)**"
      ],
      "metadata": {
        "id": "I3LcwYHJdABD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnablePositionalEncodingParam(nn.Module):\n",
        "    def __init__(self, d_model: int, max_seq_len: int = 512, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Create learnable parameters directly\n",
        "        # Shape: [max_seq_len, d_model]\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(max_seq_len, d_model) * (d_model ** -0.5)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "        Returns:\n",
        "            [batch_size, seq_len, d_model] with positional info\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Slice the positional embeddings to match input sequence length\n",
        "        # This automatically handles variable-length sequences up to max_seq_len\n",
        "        pos_enc = self.pos_embedding[:seq_len, :]  # [seq_len, d_model]\n",
        "\n",
        "        return self.dropout(x + pos_enc)"
      ],
      "metadata": {
        "id": "AIsZiO5tcLms"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SIMPLE SELF-ATTENTION MODULE**"
      ],
      "metadata": {
        "id": "nAUtVc3Tc7oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic multi-head self-attention for demonstration.\n",
        "    Shows how positional encoding integrates with attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = self.d_k ** -0.5\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model] - input with positional encoding already added\n",
        "            mask: Optional attention mask\n",
        "        Returns:\n",
        "            [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention: [batch, n_heads, seq_len, d_k]\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores: [batch, n_heads, seq_len, seq_len]\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax and dropout\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values: [batch, n_heads, seq_len, d_k]\n",
        "        context = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # Reshape back: [batch, seq_len, d_model]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Final projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "84CHnz9-cPxq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRANSFORMER BLOCK WITH LEARNABLE POSITIONAL ENCODING**"
      ],
      "metadata": {
        "id": "Sztd4u-Uex90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single transformer encoder block with Pre-LN (more stable training).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = SimpleSelfAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),  # Modern activation, smoother than ReLU\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Pre-LN: normalize before sublayers (more stable training)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        # Pre-LN Self-Attention with residual\n",
        "        x = x + self.attention(self.norm1(x), mask)\n",
        "\n",
        "        # Pre-LN Feed-Forward with residual\n",
        "        x = x + self.feed_forward(self.norm2(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8K4F_KHddB-F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMPLETE TRANSFORMER MODEL**"
      ],
      "metadata": {
        "id": "weOTrMjfeuKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleTransformerClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete transformer for sequence classification.\n",
        "    Demonstrates integration of learnable positional encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 256,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 4,\n",
        "        d_ff: int = 1024,\n",
        "        max_seq_len: int = 128,\n",
        "        n_classes: int = 2,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Token embedding\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0, std=d_model ** -0.5)\n",
        "\n",
        "        # LEARNABLE POSITIONAL ENCODING - the key component\n",
        "        self.positional_encoding = LearnablePositionalEncoding(\n",
        "            d_model=d_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Stack of transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Final normalization and classification head\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, n_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len] - token indices\n",
        "            mask: Optional padding mask\n",
        "        Returns:\n",
        "            [batch_size, n_classes] - class logits\n",
        "        \"\"\"\n",
        "        # Token embedding with scaling (from original Transformer paper)\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add learnable positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Final normalization\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Global average pooling over sequence dimension\n",
        "        # Alternative: use [CLS] token like BERT\n",
        "        x = x.mean(dim=1)  # [batch_size, d_model]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(x)  # [batch_size, n_classes]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "KoGIgj4ydM9u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DUMMY DATASET FOR DEMONSTRATION**"
      ],
      "metadata": {
        "id": "bm2gsQyFeqKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummySequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Synthetic dataset for demonstration.\n",
        "\n",
        "    Task: Classify sequences based on whether they contain more\n",
        "    tokens from the first half of vocabulary (class 0) or second half (class 1).\n",
        "\n",
        "    This requires the model to understand token distributions across positions,\n",
        "    making positional encoding important for learning patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, seq_len: int, num_samples: int):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # Generate data\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            # Randomly choose class\n",
        "            label = torch.randint(0, 2, (1,)).item()\n",
        "\n",
        "            if label == 0:\n",
        "                # Class 0: More tokens from first half of vocabulary\n",
        "                # with some position-dependent patterns\n",
        "                seq = torch.randint(1, vocab_size // 2, (seq_len,))\n",
        "                # Add some noise\n",
        "                noise_positions = torch.randint(0, seq_len, (seq_len // 4,))\n",
        "                seq[noise_positions] = torch.randint(vocab_size // 2, vocab_size, (len(noise_positions),))\n",
        "            else:\n",
        "                # Class 1: More tokens from second half\n",
        "                seq = torch.randint(vocab_size // 2, vocab_size, (seq_len,))\n",
        "                # Add some noise\n",
        "                noise_positions = torch.randint(0, seq_len, (seq_len // 4,))\n",
        "                seq[noise_positions] = torch.randint(1, vocab_size // 2, (len(noise_positions),))\n",
        "\n",
        "            # Add position-dependent signal: early positions matter more for class\n",
        "            if label == 0:\n",
        "                seq[:seq_len // 4] = torch.randint(1, vocab_size // 4, (seq_len // 4,))\n",
        "            else:\n",
        "                seq[:seq_len // 4] = torch.randint(3 * vocab_size // 4, vocab_size, (seq_len // 4,))\n",
        "\n",
        "            self.data.append(seq)\n",
        "            self.labels.append(label)\n",
        "\n",
        "        self.data = torch.stack(self.data)\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "epcs91m2dT6D"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Function**"
      ],
      "metadata": {
        "id": "duKJ8PudeiUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer():\n",
        "    \"\"\"\n",
        "    Complete training loop demonstrating learnable positional encoding.\n",
        "    \"\"\"\n",
        "    # Configuration\n",
        "    VOCAB_SIZE = 1000\n",
        "    D_MODEL = 128\n",
        "    N_HEADS = 4\n",
        "    N_LAYERS = 3\n",
        "    D_FF = 512\n",
        "    MAX_SEQ_LEN = 64\n",
        "    N_CLASSES = 2\n",
        "    DROPOUT = 0.1\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 20\n",
        "    LEARNING_RATE = 3e-4\n",
        "\n",
        "    TRAIN_SAMPLES = 2000\n",
        "    VAL_SAMPLES = 500\n",
        "\n",
        "    # Device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = DummySequenceDataset(VOCAB_SIZE, MAX_SEQ_LEN, TRAIN_SAMPLES)\n",
        "    val_dataset = DummySequenceDataset(VOCAB_SIZE, MAX_SEQ_LEN, VAL_SAMPLES)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Create model\n",
        "    model = SimpleTransformerClassifier(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        max_seq_len=MAX_SEQ_LEN,\n",
        "        n_classes=N_CLASSES,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    pos_params = model.positional_encoding.pos_embedding.weight.numel()\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Positional encoding parameters: {pos_params:,} ({100*pos_params/total_params:.1f}%)\")\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=LEARNING_RATE,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1  # 10% warmup\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Starting training...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        for batch_idx, (sequences, labels) in enumerate(train_loader):\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(sequences)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (important for transformer stability)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = logits.max(1)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_loader:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "                logits = model(sequences)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = logits.max(1)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "        # Update best\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.1f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.1f}%\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Training complete! Best validation accuracy: {best_val_acc:.1f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Demonstrate positional encoding values learned\n",
        "    print(\"\\n--- Learned Positional Encoding Analysis ---\")\n",
        "    with torch.no_grad():\n",
        "        pos_weights = model.positional_encoding.pos_embedding.weight.cpu()\n",
        "        print(f\"Positional embedding shape: {pos_weights.shape}\")\n",
        "        print(f\"Position 0 mean: {pos_weights[0].mean():.4f}, std: {pos_weights[0].std():.4f}\")\n",
        "        print(f\"Position 31 mean: {pos_weights[31].mean():.4f}, std: {pos_weights[31].std():.4f}\")\n",
        "\n",
        "        # Compute similarity between adjacent positions\n",
        "        similarities = []\n",
        "        for i in range(min(10, MAX_SEQ_LEN - 1)):\n",
        "            sim = F.cosine_similarity(\n",
        "                pos_weights[i].unsqueeze(0),\n",
        "                pos_weights[i+1].unsqueeze(0)\n",
        "            ).item()\n",
        "            similarities.append(sim)\n",
        "        print(f\"Cosine similarities between adjacent positions (0-9): {[f'{s:.3f}' for s in similarities]}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "tfuN_P2YeCBA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Learnable Positional Encoding Demonstration\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Run training\n",
        "    trained_model = train_transformer()\n",
        "\n",
        "    # Quick inference example\n",
        "    print(\"\\n--- Inference Example ---\")\n",
        "    device = next(trained_model.parameters()).device\n",
        "    trained_model.eval()\n",
        "\n",
        "    # Create a sample sequence\n",
        "    sample_seq = torch.randint(1, 500, (1, 64)).to(device)  # Class 0 range\n",
        "    with torch.no_grad():\n",
        "        output = trained_model(sample_seq)\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        pred_class = output.argmax(dim=-1).item()\n",
        "\n",
        "    print(f\"Sample sequence (tokens 1-500 dominant)\")\n",
        "    print(f\"Predicted class: {pred_class}\")\n",
        "    print(f\"Class probabilities: {probs[0].cpu().numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zDfxpDPeaPh",
        "outputId": "dcdaa297-cdfc-4301-bd8b-fcf2e32c61cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnable Positional Encoding Demonstration\n",
            "============================================================\n",
            "Using device: cuda\n",
            "Total parameters: 731,522\n",
            "Positional encoding parameters: 8,192 (1.1%)\n",
            "\n",
            "============================================================\n",
            "Starting training...\n",
            "============================================================\n",
            "Epoch  1/20 | Train Loss: 0.6256 | Train Acc: 70.6% | Val Loss: 0.2975 | Val Acc: 99.8%\n",
            "Epoch  2/20 | Train Loss: 0.0365 | Train Acc: 99.9% | Val Loss: 0.0003 | Val Acc: 100.0%\n",
            "Epoch  3/20 | Train Loss: 0.0005 | Train Acc: 100.0% | Val Loss: 0.0003 | Val Acc: 100.0%\n",
            "Epoch  4/20 | Train Loss: 0.0003 | Train Acc: 100.0% | Val Loss: 0.0002 | Val Acc: 100.0%\n",
            "Epoch  5/20 | Train Loss: 0.0003 | Train Acc: 100.0% | Val Loss: 0.0002 | Val Acc: 100.0%\n",
            "Epoch  6/20 | Train Loss: 0.0002 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch  7/20 | Train Loss: 0.0002 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch  8/20 | Train Loss: 0.0002 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch  9/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 10/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 11/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 12/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 13/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 14/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 15/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 16/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 17/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 18/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 19/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "Epoch 20/20 | Train Loss: 0.0001 | Train Acc: 100.0% | Val Loss: 0.0001 | Val Acc: 100.0%\n",
            "============================================================\n",
            "Training complete! Best validation accuracy: 100.0%\n",
            "============================================================\n",
            "\n",
            "--- Learned Positional Encoding Analysis ---\n",
            "Positional embedding shape: torch.Size([64, 128])\n",
            "Position 0 mean: 0.0034, std: 0.0843\n",
            "Position 31 mean: 0.0032, std: 0.0839\n",
            "Cosine similarities between adjacent positions (0-9): ['-0.104', '0.187', '-0.070', '-0.026', '0.128', '-0.033', '-0.104', '0.027', '-0.146', '-0.123']\n",
            "\n",
            "--- Inference Example ---\n",
            "Sample sequence (tokens 1-500 dominant)\n",
            "Predicted class: 0\n",
            "Class probabilities: [9.999306e-01 6.931682e-05]\n"
          ]
        }
      ]
    }
  ]
}